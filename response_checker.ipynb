{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c22e40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph , START , END\n",
    "from langchain import runnables\n",
    "from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint , HuggingFacePipeline\n",
    "import os\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel , Field\n",
    "from langchain.output_parsers import StructuredOutputParser , PydanticOutputParser\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10331bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a13eb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('HUGGINGFACEHUB_API_KEY')\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"google/gemma-2-2b-it\",\n",
    "    task= \"text-generation\",\n",
    "    huggingfacehub_api_token=api_key,  # ✅ THIS IS CRITICAL!\n",
    "\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bfc5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result type: <class '__main__.Schema'>\n",
      "Result content: sentiment='negative' score=2\n",
      "{'sentiment': 'negative', 'score': 2, 'review': 'your product is not good at all', 'result': AIMessage(content='```json\\n{\\n  \"issue_type\": \"Product Quality\",\\n  \"tone\": \"Polite and Concerned\",\\n  \"urgency\": \"Non-Urgent\"\\n}\\n``` \\n\\n**Explanation:**\\n\\n* **issue_type:** Clearly identifies the problem as a concern relating to the quality of the product.\\n* **tone:**  \"Polite\" reflects a respectful and understanding stance, acknowledging the customer\\'s frustration. \"Concerned\" conveys empathy and a desire to understand the reasons for dissatisfaction.\\n* **urgency:** \"Non-Urgent\" signifies that while the issue is a concern, it\\'s not an immediate problem that requires a rushed response. \\n\\n\\n**Example response:**\\n\\n\"Thank you for sharing your feedback about the product. We are sorry to hear that it didn’t meet your expectations. We take all feedback seriously and strive to provide our customers with products we are proud to offer. We would like to learn more about your experience. Could you please tell us more about what aspects of the product you were dissatisfied with? We\\'ll ensure the feedback is shared with our team, and take appropriate steps towards a resolution when requested.\" \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 242, 'prompt_tokens': 101, 'total_tokens': 343}, 'model_name': 'google/gemma-2-2b-it', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6632533c-58f6-4982-92dc-59ac159b7b3a-0', usage_metadata={'input_tokens': 101, 'output_tokens': 242, 'total_tokens': 343})}\n",
      "Sentiment: negative\n",
      "Score: 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Pregel.invoke() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 177\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Pregel.invoke() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "from typing import Literal \n",
    "class Schema(BaseModel):\n",
    "\n",
    "    sentiment: Literal[\"positive\",\"negative\"] = Field(description='Tell me the review is negative or positive....')\n",
    "    score: int = Field(descripxtion='Score out of 10 how worst the review the more worst is 10 out of 10', ge=0, le=10)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object = Schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template= 'give the review and clarify the review  {review} and give the output in sentiment is it positive or negative and what is worst review score \\n {format_instruction}',\n",
    "    input_variables= ['review'],\n",
    "    partial_variables = {'format_instruction' : parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt = template.invoke({'your product is normal but i faced some diffuculties in it try to improve it '})\n",
    "\n",
    "sentiment_chain = template | model | parser\n",
    "\n",
    "final_result = sentiment_chain.invoke(prompt)\n",
    "\n",
    "class Finalstate(TypedDict):\n",
    "    sentiment: str      # ✅ Add this\n",
    "    score: int  \n",
    "    review : str\n",
    "    issue_type : str\n",
    "    tone : str\n",
    "    urgency : str\n",
    "    result : str\n",
    "\n",
    "def find_sentiment(state: Finalstate):\n",
    "    review_text = state['review']\n",
    "    prompt_input = {'review': review_text}\n",
    "\n",
    "    result = sentiment_chain.invoke(prompt_input)  # returns a Pydantic object (Schema)\n",
    "    print(\"Result type:\", type(result))\n",
    "    print(\"Result content:\", result)\n",
    "\n",
    "    return {\n",
    "        'sentiment': result.sentiment,  # ✅ object attribute access\n",
    "        'score': result.score\n",
    "    }\n",
    "   \n",
    "\n",
    "\n",
    "def run_diagnosis(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "\n",
    "    1. Issue Type – is it about product quality, delivery, price, etc.?\n",
    "    2. Tone – is the tone aggressive, soft, neutral, unsatisfied, etc.?\n",
    "    3. Urgency – does the user express urgency or not?\n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def run_diagnosis(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "\n",
    "    1. Issue Type – is it about product quality, delivery, price, etc.?\n",
    "    2. Tone – is the tone aggressive, soft, neutral, unsatisfied, etc.?\n",
    "    3. Urgency – does the user express urgency or not?\n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def negative_response(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three :\n",
    "     Give the reply for  experience in the polite way as polite as possible ... and tell them on next time we take care about this \n",
    "    \n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def positive_response(state: Finalstate):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "     \n",
    "    Give the reply for positive experience in the polite way as polite as possible ... and tell them on next time we take care about this \n",
    "     \n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \n",
    "      \"reply\": \"<reply>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def generate_reply(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful customer support assistant. Based on the extracted feedback details below, write a professional and empathetic reply to the customer.\n",
    "\n",
    "Details:\n",
    "- Issue Type: {state['result']['issue_type']}\n",
    "- Tone: {state['result']['tone']}\n",
    "- Urgency: {state['result']['urgency']}\n",
    "\n",
    "Guidelines:\n",
    "- If tone is angry/frustrated/disappointed → start with an apology.\n",
    "- If urgency is urgent → offer quick help or escalation.\n",
    "- Mention the issue type in the reply.\n",
    "- Keep it clear, respectful, and reassuring.\n",
    "\n",
    "Return only the customer reply message. No formatting, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)\n",
    "    return {'reply': result}\n",
    "\n",
    "def check_condition(state: Finalstate) -> Literal[\"run_diagnosis\", \"positive_response\"]:\n",
    "\n",
    "    if state['sentiment'] == \"positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\"\n",
    "    \n",
    "graph = StateGraph(Finalstate)\n",
    "\n",
    "graph.add_node(\"find_sentiment\" , find_sentiment)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\" , negative_response)\n",
    "graph.add_node(\"positive_response\", positive_response)\n",
    "\n",
    "graph.add_edge(START , \"find_sentiment\")\n",
    "graph.add_conditional_edges(\"find_sentiment\", check_condition)\n",
    "graph.add_edge(\"run_diagnosis\", \"negative_response\")\n",
    "graph.add_edge(\"negative_response\", END)\n",
    "graph.add_edge(\"positive_response\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "initial_state = {\"review\": \"your product is not good at all\"}\n",
    "\n",
    "final_result = workflow.invoke(initial_state)\n",
    "\n",
    "print(final_result)\n",
    "\n",
    "print(\"Sentiment:\", final_result['sentiment'])\n",
    "print(\"Score:\", final_result['score'])\n",
    "\n",
    "workflow.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d995657",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object = Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b7e687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template= 'give the review and clarify the review  {review} and give the output in sentiment is it positive or negative and what is worst review score \\n {format_instruction}',\n",
    "    input_variables= ['review'],\n",
    "    partial_variables = {'format_instruction' : parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt = template.invoke({'your product is normal but i faced some diffuculties in it try to improve it '})\n",
    "\n",
    "sentiment_chain = template | model | parser\n",
    "\n",
    "final_result = sentiment_chain.invoke(prompt)\n",
    "\n",
    "class Finalstate(TypedDict):\n",
    "\n",
    "    review : str\n",
    "    issue_type : str\n",
    "    tone : str\n",
    "    urgency : str\n",
    "    result : str\n",
    "\n",
    "def find_sentiment(state: Finalstate):\n",
    "    review_text = state['review']\n",
    "    prompt_input = {'review': review_text}\n",
    "\n",
    "    result = sentiment_chain.invoke(prompt_input)  # returns a Pydantic object (Schema)\n",
    "    print(\"Result type:\", type(result))\n",
    "    print(\"Result content:\", result)\n",
    "\n",
    "    return {\n",
    "        'sentiment': result.sentiment,  # ✅ object attribute access\n",
    "        'score': result.score\n",
    "    }\n",
    "   \n",
    "\n",
    "\n",
    "def run_diagnosis(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "\n",
    "    1. Issue Type – is it about product quality, delivery, price, etc.?\n",
    "    2. Tone – is the tone aggressive, soft, neutral, unsatisfied, etc.?\n",
    "    3. Urgency – does the user express urgency or not?\n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def run_diagnosis(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "\n",
    "    1. Issue Type – is it about product quality, delivery, price, etc.?\n",
    "    2. Tone – is the tone aggressive, soft, neutral, unsatisfied, etc.?\n",
    "    3. Urgency – does the user express urgency or not?\n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def negative_response(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three :\n",
    "    Generate the particular\n",
    "    \n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \"issue_type\": \"<type>\",\n",
    "      \"tone\": \"<tone>\",\n",
    "      \"urgency\": \"<urgency>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def positive_response(state: Finalstate):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer review and answer these three things:\n",
    "     \n",
    "    Give the reply for positive experience in the polite way as polite as possible ... and tell them on next time we take care about this \n",
    "     \n",
    "\n",
    "    Review: {state['review']}\n",
    "\n",
    "    Respond in this JSON format:\n",
    "    {{\n",
    "      \n",
    "      \"reply\": \"<reply>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)  # Make sure 'model' is defined globally or passed in\n",
    "    return {'result': result}\n",
    "\n",
    "def generate_reply(state: Finalstate):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful customer support assistant. Based on the extracted feedback details below, write a professional and empathetic reply to the customer.\n",
    "\n",
    "Details:\n",
    "- Issue Type: {state['result']['issue_type']}\n",
    "- Tone: {state['result']['tone']}\n",
    "- Urgency: {state['result']['urgency']}\n",
    "\n",
    "Guidelines:\n",
    "- If tone is angry/frustrated/disappointed → start with an apology.\n",
    "- If urgency is urgent → offer quick help or escalation.\n",
    "- Mention the issue type in the reply.\n",
    "- Keep it clear, respectful, and reassuring.\n",
    "\n",
    "Return only the customer reply message. No formatting, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "    result = model.invoke(prompt)\n",
    "    return {'reply': result}\n",
    "\n",
    "def check_condition(state: Finalstate) -> Literal[\"run_diagnosis\", \"positive_response\"]:\n",
    "\n",
    "    if state['sentiment'] == \"positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\"\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "581bb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(Finalstate)\n",
    "\n",
    "graph.add_node(\"find_sentiment\" , find_sentiment)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\" , negative_response)\n",
    "graph.add_node(\"positive_response\", positive_response)\n",
    "\n",
    "graph.add_edge(START , \"find_sentiment\")\n",
    "graph.add_conditional_edges(\"find_sentiment\", check_condition)\n",
    "graph.add_edge(\"run_diagnosis\", \"negative_response\")\n",
    "graph.add_edge(\"negative_response\", END)\n",
    "graph.add_edge(\"positive_response\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "initial_state = {\"review\": \"your product is not good at all\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceefcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844a2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203cfb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f1505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f6930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61337100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72898914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
